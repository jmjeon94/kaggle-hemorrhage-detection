{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialHmData(Dataset):\n",
    "    def __init__(self, feature_path, df_path):\n",
    "        self.feature_df = pd.read_csv(feature_path)\n",
    "        self.ref_df = pd.read_csv(df_path)\n",
    "        self.person_ids = self.ref_df.study_instance_uid.unique()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        current_person_id = self.person_ids[index]\n",
    "        filenames = self.ref_df[self.ref_df.study_instance_uid==current_person_id].filename\n",
    "\n",
    "        df_current_person = self.feature_df[self.feature_df.filename.isin(filenames)]\n",
    "        label = df_current_person.iloc[:,1:7]\n",
    "        features = df_current_person.iloc[:,7:]\n",
    "        \n",
    "        \n",
    "        \n",
    "        return label, features\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ref_df.study_instance_uid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SequentialHmData(feature_path='./dataset/train_features.csv', df_path='./dataset/train.csv')\n",
    "valid_dataset = SequentialHmData(feature_path='./dataset/valid_features.csv', df_path='./dataset/valid.csv')\n",
    "test_dataset = SequentialHmData(feature_path='./dataset/test_features.csv', df_path='./dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "      <th>label_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.015820</td>\n",
       "      <td>0.008004</td>\n",
       "      <td>0.018745</td>\n",
       "      <td>0.013196</td>\n",
       "      <td>0.033904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001421</td>\n",
       "      <td>0.016012</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>0.019219</td>\n",
       "      <td>0.013521</td>\n",
       "      <td>0.036777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.016703</td>\n",
       "      <td>0.007739</td>\n",
       "      <td>0.018858</td>\n",
       "      <td>0.013182</td>\n",
       "      <td>0.035088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001460</td>\n",
       "      <td>0.016845</td>\n",
       "      <td>0.007683</td>\n",
       "      <td>0.019069</td>\n",
       "      <td>0.013656</td>\n",
       "      <td>0.036148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001424</td>\n",
       "      <td>0.016073</td>\n",
       "      <td>0.007486</td>\n",
       "      <td>0.018949</td>\n",
       "      <td>0.013299</td>\n",
       "      <td>0.035246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.016130</td>\n",
       "      <td>0.007555</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.013492</td>\n",
       "      <td>0.035669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001432</td>\n",
       "      <td>0.016321</td>\n",
       "      <td>0.007848</td>\n",
       "      <td>0.019178</td>\n",
       "      <td>0.013795</td>\n",
       "      <td>0.036316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.016626</td>\n",
       "      <td>0.007837</td>\n",
       "      <td>0.018888</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.035537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.016455</td>\n",
       "      <td>0.007539</td>\n",
       "      <td>0.019107</td>\n",
       "      <td>0.013448</td>\n",
       "      <td>0.035402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001460</td>\n",
       "      <td>0.016220</td>\n",
       "      <td>0.007850</td>\n",
       "      <td>0.018771</td>\n",
       "      <td>0.013163</td>\n",
       "      <td>0.036393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001449</td>\n",
       "      <td>0.015802</td>\n",
       "      <td>0.007526</td>\n",
       "      <td>0.018525</td>\n",
       "      <td>0.013338</td>\n",
       "      <td>0.035039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.015720</td>\n",
       "      <td>0.007586</td>\n",
       "      <td>0.018240</td>\n",
       "      <td>0.012824</td>\n",
       "      <td>0.033641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.015855</td>\n",
       "      <td>0.007488</td>\n",
       "      <td>0.019097</td>\n",
       "      <td>0.013554</td>\n",
       "      <td>0.036381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.016023</td>\n",
       "      <td>0.007605</td>\n",
       "      <td>0.018294</td>\n",
       "      <td>0.012762</td>\n",
       "      <td>0.034181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.015631</td>\n",
       "      <td>0.007371</td>\n",
       "      <td>0.018097</td>\n",
       "      <td>0.012580</td>\n",
       "      <td>0.035676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.001356</td>\n",
       "      <td>0.015373</td>\n",
       "      <td>0.007201</td>\n",
       "      <td>0.018564</td>\n",
       "      <td>0.013011</td>\n",
       "      <td>0.035263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.015831</td>\n",
       "      <td>0.007527</td>\n",
       "      <td>0.018081</td>\n",
       "      <td>0.012478</td>\n",
       "      <td>0.034347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.001297</td>\n",
       "      <td>0.015489</td>\n",
       "      <td>0.007099</td>\n",
       "      <td>0.018297</td>\n",
       "      <td>0.012855</td>\n",
       "      <td>0.034009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.015033</td>\n",
       "      <td>0.007302</td>\n",
       "      <td>0.018429</td>\n",
       "      <td>0.012622</td>\n",
       "      <td>0.034401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.001360</td>\n",
       "      <td>0.015769</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>0.018627</td>\n",
       "      <td>0.012723</td>\n",
       "      <td>0.035430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.001338</td>\n",
       "      <td>0.015383</td>\n",
       "      <td>0.007434</td>\n",
       "      <td>0.018320</td>\n",
       "      <td>0.012331</td>\n",
       "      <td>0.034396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.015802</td>\n",
       "      <td>0.007483</td>\n",
       "      <td>0.018414</td>\n",
       "      <td>0.012721</td>\n",
       "      <td>0.035638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.015650</td>\n",
       "      <td>0.007403</td>\n",
       "      <td>0.017933</td>\n",
       "      <td>0.012948</td>\n",
       "      <td>0.035780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.015665</td>\n",
       "      <td>0.007642</td>\n",
       "      <td>0.018660</td>\n",
       "      <td>0.012689</td>\n",
       "      <td>0.035584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.016344</td>\n",
       "      <td>0.007570</td>\n",
       "      <td>0.018812</td>\n",
       "      <td>0.013136</td>\n",
       "      <td>0.036255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.015128</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>0.017993</td>\n",
       "      <td>0.012936</td>\n",
       "      <td>0.035067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.015261</td>\n",
       "      <td>0.007387</td>\n",
       "      <td>0.018830</td>\n",
       "      <td>0.013520</td>\n",
       "      <td>0.035899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.016072</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.019238</td>\n",
       "      <td>0.013428</td>\n",
       "      <td>0.035287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.015526</td>\n",
       "      <td>0.007486</td>\n",
       "      <td>0.018569</td>\n",
       "      <td>0.013449</td>\n",
       "      <td>0.035779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.015956</td>\n",
       "      <td>0.007765</td>\n",
       "      <td>0.018877</td>\n",
       "      <td>0.013682</td>\n",
       "      <td>0.036682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.016395</td>\n",
       "      <td>0.007788</td>\n",
       "      <td>0.018450</td>\n",
       "      <td>0.013027</td>\n",
       "      <td>0.034443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.016712</td>\n",
       "      <td>0.008096</td>\n",
       "      <td>0.018813</td>\n",
       "      <td>0.013961</td>\n",
       "      <td>0.037225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.016826</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>0.019192</td>\n",
       "      <td>0.013568</td>\n",
       "      <td>0.036794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.017095</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>0.020052</td>\n",
       "      <td>0.014082</td>\n",
       "      <td>0.035990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.001471</td>\n",
       "      <td>0.016837</td>\n",
       "      <td>0.008039</td>\n",
       "      <td>0.018846</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>0.036576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.016373</td>\n",
       "      <td>0.008002</td>\n",
       "      <td>0.019204</td>\n",
       "      <td>0.013414</td>\n",
       "      <td>0.036088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label_0   label_1   label_2   label_3   label_4   label_5\n",
       "0   0.001414  0.015820  0.008004  0.018745  0.013196  0.033904\n",
       "1   0.001421  0.016012  0.008033  0.019219  0.013521  0.036777\n",
       "2   0.001357  0.016703  0.007739  0.018858  0.013182  0.035088\n",
       "3   0.001460  0.016845  0.007683  0.019069  0.013656  0.036148\n",
       "4   0.001424  0.016073  0.007486  0.018949  0.013299  0.035246\n",
       "5   0.001428  0.016130  0.007555  0.018800  0.013492  0.035669\n",
       "6   0.001432  0.016321  0.007848  0.019178  0.013795  0.036316\n",
       "7   0.001439  0.016626  0.007837  0.018888  0.013559  0.035537\n",
       "8   0.001370  0.016455  0.007539  0.019107  0.013448  0.035402\n",
       "9   0.001460  0.016220  0.007850  0.018771  0.013163  0.036393\n",
       "10  0.001449  0.015802  0.007526  0.018525  0.013338  0.035039\n",
       "11  0.001383  0.015720  0.007586  0.018240  0.012824  0.033641\n",
       "12  0.001379  0.015855  0.007488  0.019097  0.013554  0.036381\n",
       "13  0.001350  0.016023  0.007605  0.018294  0.012762  0.034181\n",
       "14  0.001391  0.015631  0.007371  0.018097  0.012580  0.035676\n",
       "15  0.001356  0.015373  0.007201  0.018564  0.013011  0.035263\n",
       "16  0.001293  0.015831  0.007527  0.018081  0.012478  0.034347\n",
       "17  0.001297  0.015489  0.007099  0.018297  0.012855  0.034009\n",
       "18  0.001307  0.015033  0.007302  0.018429  0.012622  0.034401\n",
       "19  0.001360  0.015769  0.007380  0.018627  0.012723  0.035430\n",
       "20  0.001338  0.015383  0.007434  0.018320  0.012331  0.034396\n",
       "21  0.001358  0.015802  0.007483  0.018414  0.012721  0.035638\n",
       "22  0.001311  0.015650  0.007403  0.017933  0.012948  0.035780\n",
       "23  0.001332  0.015665  0.007642  0.018660  0.012689  0.035584\n",
       "24  0.001347  0.016344  0.007570  0.018812  0.013136  0.036255\n",
       "25  0.001387  0.015128  0.007273  0.017993  0.012936  0.035067\n",
       "26  0.001361  0.015261  0.007387  0.018830  0.013520  0.035899\n",
       "27  0.001399  0.016072  0.007365  0.019238  0.013428  0.035287\n",
       "28  0.001388  0.015526  0.007486  0.018569  0.013449  0.035779\n",
       "29  0.001362  0.015956  0.007765  0.018877  0.013682  0.036682\n",
       "30  0.001404  0.016395  0.007788  0.018450  0.013027  0.034443\n",
       "31  0.001438  0.016712  0.008096  0.018813  0.013961  0.037225\n",
       "32  0.001433  0.016826  0.008326  0.019192  0.013568  0.036794\n",
       "33  0.001481  0.017095  0.008219  0.020052  0.014082  0.035990\n",
       "34  0.001471  0.016837  0.008039  0.018846  0.013999  0.036576\n",
       "35  0.001462  0.016373  0.008002  0.019204  0.013414  0.036088"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 6, 1, 35]), torch.Size([10, 6, 1, 35]))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SequenceModel(nn.Module):\n",
    "    def __init__(self, ch_in=1024):\n",
    "        super(SequenceModel, self).__init__()\n",
    "        drop_out = 0.5\n",
    "        hidden = 96\n",
    "        lstm_layers = 2\n",
    "        feature_num=1\n",
    "        ratio = 1\n",
    "        self.ratio=ratio\n",
    "        \n",
    "        # seq model 1\n",
    "        self.fea_conv = nn.Sequential(nn.Dropout2d(drop_out),\n",
    "                                      nn.Conv2d(ch_in, 512, kernel_size=(1, 1), stride=(1,1),padding=(0,0), bias=False),\n",
    "                                      nn.BatchNorm2d(512),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout2d(drop_out),\n",
    "                                      nn.Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), bias=False),\n",
    "                                      nn.BatchNorm2d(128),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout2d(drop_out),\n",
    "                                      )\n",
    "\n",
    "        self.fea_first_final = nn.Sequential(nn.Conv2d(128*feature_num, 6, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), bias=True))\n",
    "\n",
    "        # # bidirectional GRU\n",
    "        self.hidden_fea = hidden\n",
    "        self.fea_lstm = nn.GRU(128*feature_num, self.hidden_fea, num_layers=lstm_layers, batch_first=True, bidirectional=True)\n",
    "        self.fea_lstm_final = nn.Sequential(nn.Conv2d(1, 6, kernel_size=(1, self.hidden_fea*2), stride=(1, 1), padding=(0, 0), dilation=1, bias=True))\n",
    "        \n",
    "        \n",
    "        # seq model 2\n",
    "        self.conv_first = nn.Sequential(nn.Conv2d(12, 128*ratio, kernel_size=(5, 1), stride=(1,1),padding=(2,0),dilation=1, bias=False),\n",
    "                                        nn.BatchNorm2d(128*ratio),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Conv2d(128*ratio, 64*ratio, kernel_size=(3, 1), stride=(1, 1), padding=(2, 0),dilation=2, bias=False),\n",
    "                                        nn.BatchNorm2d(64*ratio),\n",
    "                                        nn.ReLU())\n",
    "\n",
    "        self.conv_res = nn.Sequential(nn.Conv2d(64 * ratio, 64 * ratio, kernel_size=(3, 1), stride=(1, 1),padding=(4, 0),dilation=4, bias=False),\n",
    "                                      nn.BatchNorm2d(64 * ratio),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Conv2d(64 * ratio, 64 * ratio, kernel_size=(3, 1), stride=(1, 1),padding=(2, 0),dilation=2, bias=False),\n",
    "                                      nn.BatchNorm2d(64 * ratio),\n",
    "                                      nn.ReLU())\n",
    "\n",
    "        self.conv_final = nn.Sequential(nn.Conv2d(64*ratio, 1, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), dilation=1,bias=False))\n",
    "\n",
    "        # bidirectional GRU\n",
    "        self.hidden = hidden\n",
    "        self.lstm = nn.GRU(64*ratio, self.hidden, num_layers=lstm_layers, batch_first=True, bidirectional=True)\n",
    "        self.final = nn.Sequential(nn.Conv2d(1, 6, kernel_size=(1, self.hidden*2), stride=(1, 1), padding=(0, 0), dilation=1, bias=True))\n",
    "\n",
    "\n",
    "    def forward(self, features, x):\n",
    "        \n",
    "        batch_size, _, _, _ = features.shape\n",
    "        \n",
    "        # stem_fc\n",
    "        x_fc = self.fea_conv(features) # (N, LenFeat, 1, LenSeq)\n",
    "        \n",
    "        # fc\n",
    "        out11 = self.fea_first_final(x_fc) # (N, 6, 1, LenSeq)\n",
    "\n",
    "        # lstm\n",
    "        x_lstm, _ = self.fea_lstm(x_fc.view(batch_size, -1, 128)) # (N, LenSeq,192)\n",
    "        x_lstm = x_lstm.reshape(batch_size, 1, -1, self.hidden_fea*2) # (N, 1, LenSeq, 192)\n",
    "        out12 = self.fea_lstm_final(x_lstm) # (N, 6, 1, LenSeq)\n",
    "        out12 = out12.permute(0,1,3,2)\n",
    "        \n",
    "        # seq1 output\n",
    "        out1 = out11+out12\n",
    "        out1_sigmoid = torch.sigmoid(out1)\n",
    "        \n",
    "        # concat cnn out, seq1 out\n",
    "        x = torch.cat([x, out1_sigmoid], dim=1)\n",
    "        \n",
    "        # stem_fc\n",
    "        x = self.conv_first(x)\n",
    "        x = self.conv_res(x)\n",
    "        \n",
    "        # fc\n",
    "        out21 = self.conv_final(x)\n",
    "        \n",
    "        # lstm\n",
    "        x, _ = self.lstm(x.view(batch_size, -1, 64))\n",
    "        x = x.reshape(batch_size, 1, -1, self.hidden*2)\n",
    "        out22 = self.final(x)\n",
    "        out22 = out22.permute(0,1,3,2)\n",
    "        \n",
    "        # seq2 output\n",
    "        out2 = out21 + out22\n",
    "        \n",
    "        return out1, out2\n",
    "    \n",
    "model = SequenceModel(ch_in=1024)\n",
    "f = np.zeros([10,1024,1,35])\n",
    "x = np.zeros([10, 6, 1, 35])\n",
    "\n",
    "feature = torch.from_numpy(f).float()\n",
    "inputs = torch.from_numpy(x).float()\n",
    "\n",
    "o1, o2 = model(feature, inputs)\n",
    "o1.shape, o2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.GRU(1024, 6, num_layers=2, bidirectional=True)\n",
    "input = torch.randn(100, 35, 1024) # Batch size, sequence length, feature size\n",
    "\n",
    "output, hn = rnn(input) # batch size, sequence length, output feature size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 35, 12])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
